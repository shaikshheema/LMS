# -*- coding: utf-8 -*-
"""Multiple linear regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wxKOcFhIpY5X6GBtLlRVFn5YsidPJvge

ASSUMPTIONS IN MULTIPLELINEAR REGRESSION
- 1.LINEARITY: The relationship between the predictors and the response is linear.
- 2.INDEPENDENCE: Observations are independent of each other.
- 3.HOMOSCEDASTICITY: The residuals (differences blw observed and predicted values) exhibit constant variance at all levels of the predictor
- 4.NORMAL DISTRIBUTIONOF ERRORS: The residuals of the model are normally distributed
- 5.NO MULTICOLLINEARITY: The independent variables should not be too highly correlated with each other Violations of these assumptions may lead to in efficeincy in the regression parameters and unreliable predictors
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
from statsmodels.graphics.regressionplots import influence_plot

cars=pd.read_csv("Cars.csv")
cars.head()

#rEARRANGE THE COLUMNS
cars = pd.DataFrame(cars, columns=["HP","VOL","SP","WT","MPG"])
cars.head()

"""DESCRIPTION OF COLUMNS
- HP: Horse power of car
- MPG: milege of the car
- VOL: volume of the cars(size)
- SP: Top speed of the car(Mlies per hour)
- WT: Weight of the car(ponds)
-

EDA
"""

cars.info()

#CHECK FOR MISSING VALUES
cars.isna().sum()

"""- There are no missing values
- There are 81 observations
- the data types of the columns are relevant and valid
"""

#Create a figure with two subplots (one above the other)
fig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})
#Creating a boxplot
sns.boxplot(data=cars, x='HP', ax=ax_box, orient='h')
ax_box.set(xlabel='') # Remove x Label for the boxplot
#Creating a histogram in the same x-axis
sns.histplot(data=cars, x='HP', ax=ax_hist, bins=30, kde=True, stat="density")
ax_hist.set(ylabel='Density')
#Adjust Layout
plt.tight_layout()
plt.show()

#Create a figure with two subplots (one above the other)
fig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})
#Creating a boxplot
sns.boxplot(data=cars, x='SP', ax=ax_box, orient='h')
ax_box.set(xlabel='') # Remove x Label for the boxplot
#Creating a histogram in the same x-axis
sns.histplot(data=cars, x='SP', ax=ax_hist, bins=30, kde=True, stat="density")
ax_hist.set(ylabel='Density')
#Adjust Layout
plt.tight_layout()
plt.show()

#Create a figure with two subplots (one above the other)
fig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})
#Creating a boxplot
sns.boxplot(data=cars, x='VOL', ax=ax_box, orient='h')
ax_box.set(xlabel='') # Remove x Label for the boxplot
#Creating a histogram in the same x-axis
sns.histplot(data=cars, x='VOL', ax=ax_hist, bins=30, kde=True, stat="density")
ax_hist.set(ylabel='Density')
#Adjust Layout
plt.tight_layout()
plt.show()

#Create a figure with two subplots (one above the other)
fig, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={"height_ratios": (.15, .85)})
#Creating a boxplot
sns.boxplot(data=cars, x='WT', ax=ax_box, orient='h')
ax_box.set(xlabel='') # Remove x Label for the boxplot
#Creating a histogram in the same x-axis
sns.histplot(data=cars, x='WT', ax=ax_hist, bins=30, kde=True, stat="density")
ax_hist.set(ylabel='Density')
#Adjust Layout
plt.tight_layout()
plt.show()

"""Observations from box plot
- There are some extreme values (outliers) observed in towards the right tail of SP and HP distributions.
- In VOL and wT columns,  a few outliers are observed in both tails of their distributions.
- The extreme values of cars data may have come from the specially designed nAature of cars.
- As this is multi-dimensional data, the outliers with respect to spatial dimensions may have to be consideered while building the regression model
- As this is multi-dimensional data, the outliers with respect to spatiaasl dimensions may have to be considered while building the regression model

Checking for
"""

cars[cars.duplicated()]

# Pair plot
sns.set_style(style='darkgrid')
sns.pairplot(cars)

cars.corr()

"""Observations
- high corelation values are present in HP than SP
- negative values are present in MPG.
-b/w x and y,  all the x variable are showing moderate to high corellation strength , highest being between HP and MPG.
- Therefore this dataset qualifies for building multiple linear regression model to predict MPG
- Among x columns (X1,X2,X3 and X4), , some very high coreelation strengths are observed between SP vs HP, vOL vs WT
- The high corellation among x columns is not desirable as it might lead to multicollinearity problem.

Preparing a preliminary model considering all x columns
"""

#Build model
#import statsmodels.formula.api as smf
model1 = smf.ols('MPG~WT+VOL+SP+HP', data=cars).fit()

model1.summary()

"""****R-Squared values tells about how much of variability in y is explained by X.****

OBSERVATIONS:
- The R-squared and adjusted R-Squared values are good and about 75% of variability in y is explained by X columns
- The probability value with respect to f-statistic is close to zero, indicating that all or some of x columns are significant
- The p-values for VOL and WT are higher that 5% indicating some interaction issue among themselves,  which need to be further explored

PERFORMANCE METRICS FOR MODEL1
"""

#fIND THE PERFORMNACE METRICS
#CREATE A DATA FRAME WITH ACTUAL Y AND PREDICTED Y COLUMNS

df1 = pd.DataFrame()
df1["actual_y1"]=cars["MPG"]
df1.head()

#predict for the given X data columns
pred_y1=model1.predict(cars.iloc[:,0:4])
df1["pred_y1"] = pred_y1
df1.head()

#compute the mean squared error(MSE) for model
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(df1["actual_y1"], df1["pred_y1"])
print("MSE :", mse)
print("RMSE :", np.sqrt(mse))

"""Checking for multicollinearity among X-columns using VIF method"""

cars.head()

# Compute VIF values
rsq_hp = smf.ols('HP~WT+VOL+SP',data=cars).fit().rsquared
vif_hp = 1/(1-rsq_hp)

rsq_wt = smf.ols('WT~HP+VOL+SP',data=cars).fit().rsquared
vif_wt = 1/(1-rsq_wt)

rsq_vol = smf.ols('VOL~WT+SP+HP',data=cars).fit().rsquared
vif_vol = 1/(1-rsq_vol)

rsq_sp = smf.ols('SP~WT+VOL+HP',data=cars).fit().rsquared
vif_sp = 1/(1-rsq_sp)

# Storing vif values in a data frame
d1 = {'Variables':['Hp','WT','VOL','SP'],'VIF':[vif_hp,vif_wt,vif_vol,vif_sp]}
Vif_frame = pd.DataFrame(d1)
Vif_frame

"""#### Observations
* The ideal range of VIF values shall between 1 to 10.However slightly higher values can be tolerated.
* As seen from the very high VIF values for VOL and WT,it is clear that theey are prone to multicollinearity
* Hence it is decided to drop one of the columns(either VOl ot WT) to overcome the multicollinearity.
* It is decided to drop WT and retain VOL column in further models.
"""

cars1=cars.drop("WT",axis=1)
cars1.head()

#Build model2 on cars1 dataset
import statsmodels.formula.api as smf
model2 =smf.ols("MPG~VOL+SP+HP",data=cars1).fit()

model2.summary()

# find the performance metrics for model2
# Create a data frame with actual y and predicted y columns
df2 = pd.DataFrame()
df2["actual_y2"] = cars1["MPG"]
df2.head()

# predict for the given X data columns
pred_y2 = model2.predict(cars1.iloc[:,0:4])
df2["pred_y2"] = pred_y2
df2.head()

# compute the Mean Squared Error(MSE) for model2
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(df2["actual_y2"],df2["pred_y2"])
print("MSE: ",mse)
print("RMSE: ",np.sqrt(mse))

cars1.shape

# Define variables and assign values
k=3 #no of x-columns in cars1
n=81 #no of observations(rows)
levarage_cutoff = 3*((k+1)/n)
levarage_cutoff

from statsmodels.graphics.regressionplots import influence_plot
influence_plot(model1,alpha=.05)
y=[i for i in range(-2,8)]
x=[levarage_cutoff for i in range(10)]
plt.plot(x,y,"r+")
plt.show()

cars1[cars1.index.isin([65,70,76,78,79,80])]

cars2=cars1.drop(cars1.index[[65,70,76,78,79,80]],axis=0).reset_index(drop=True)

#### Rebuild the model model
model3=smf.ols("MPG~VOL+SP+HP",data=cars2).fit()

model3.summary()

"""Build Model3 on cars2 data set"""

#Rebuild the model
model3= smf.ols('MPG~VOL+SP+HP',data = cars2).fit()

model3.summary()

"""Performance Metrices for model3"""

df3=pd.DataFrame()
df3["actual_y3"] = cars2["MPG"]
df3.head()

#predict on all x data columns
pred_y3 = model3.predict(cars2.iloc[:,0:3])
df3["pred_y3"] = pred_y3
df3.head()

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(df3["actual_y3"], df3["pred_y3"])
print("MSE :",mse)
print("RMSE :",np.sqrt(mse))

"""#### Comparison of models
                     

| Metric         | Model 1 | Model 2 | Model 3 |
|----------------|---------|---------|---------|
| R-squared      | 0.771   | 0.770   | 0.885   |
| Adj. R-squared | 0.758   | 0.761   | 0.880   |
| MSE            | 18.89   | 18.91   | 8.68    |
| RMSE           | 4.34    | 4.34    | 2.94    |


- **From the above comparison table it is observed that model3 is the best among all with superior performance metrics**

Check the validity of model assumptions for model3
"""

model3.resid

model3.fittedvalues

#the model is built with VOL, SP, HP  by ignoring WT

import statsmodels.api as sm
qqplot=sm.qqplot(model3.resid,line='q')
plt.title("Normla Q-Q plot of residuals")
plt.show()

sns.displot(model3.resid, kde = True)

def get_standardized_values( vals ):
    return (vals - vals.mean())/vals.std()

plt.figure(figsize=(6,4))
plt.scatter(get_standardized_values(model3.fittedvalues),
            get_standardized_values(model3.resid))

plt.title('Residual Plot')
plt.xlabel('Standardized Fitted values')
plt.ylabel('Standardized residual values')
plt.show()

